{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP/bsMuepCXt8HtMGnEwBYp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nobusama/scGPT/blob/main/perturbation_ipynb_%E3%81%AE%E3%82%B3%E3%83%94%E3%83%BC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f3cvxWPXm6G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fine-tuning Pre-trained Model for Perturbation Prediction\n"
      ],
      "metadata": {
        "id": "qw0QjrWNXn8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJMjLb12Y67m",
        "outputId": "9124fdb4-bbe8-422e-df82-d57ead304371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m673.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.7.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
            "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cell-gears"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UrGuPNahKhM",
        "outputId": "1615540c-105f-4de8-a651-1ceaf362d28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cell-gears\n",
            "  Downloading cell_gears-0.1.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cell-gears) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from cell-gears) (2.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cell-gears) (4.66.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from cell-gears) (1.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from cell-gears) (2.3.1+cu121)\n",
            "Collecting scanpy (from cell-gears)\n",
            "  Downloading scanpy-1.10.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from cell-gears) (3.3)\n",
            "Collecting dcor (from cell-gears)\n",
            "  Downloading dcor-0.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.10/dist-packages (from dcor->cell-gears) (0.58.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dcor->cell-gears) (1.11.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from dcor->cell-gears) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->cell-gears) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->cell-gears) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->cell-gears) (2024.1)\n",
            "Collecting anndata>=0.8 (from scanpy->cell-gears)\n",
            "  Downloading anndata-0.10.8-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from scanpy->cell-gears) (3.9.0)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy->cell-gears)\n",
            "  Downloading legacy_api_wrap-1.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy->cell-gears) (3.7.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy->cell-gears) (8.4.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scanpy->cell-gears) (24.1)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy->cell-gears) (0.5.6)\n",
            "Collecting pynndescent>=0.5 (from scanpy->cell-gears)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy->cell-gears) (0.13.1)\n",
            "Collecting session-info (from scanpy->cell-gears)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy->cell-gears) (0.14.2)\n",
            "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy->cell-gears)\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->cell-gears) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->cell-gears) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->cell-gears) (12.5.82)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata>=0.8->scanpy->cell-gears)\n",
            "  Downloading array_api_compat-1.8-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy->cell-gears) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy->cell-gears) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy->cell-gears) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy->cell-gears) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy->cell-gears) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy->cell-gears) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy->cell-gears) (3.1.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51->dcor->cell-gears) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->cell-gears) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->cell-gears) (2.1.5)\n",
            "Collecting stdlib_list (from session-info->scanpy->cell-gears)\n",
            "  Downloading stdlib_list-0.10.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->cell-gears) (1.3.0)\n",
            "Downloading cell_gears-0.1.2-py3-none-any.whl (31 kB)\n",
            "Downloading dcor-0.6-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scanpy-1.10.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.10.8-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.4/124.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.8-py3-none-any.whl (38 kB)\n",
            "Downloading stdlib_list-0.10.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: session-info\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=bbb14aa5367b87e0aaa9f21419b12d2d14a23b06b3f9008fdc6ac46bf33f2705\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "Successfully built session-info\n",
            "Installing collected packages: array-api-compat, stdlib_list, legacy-api-wrap, session-info, dcor, pynndescent, anndata, umap-learn, scanpy, cell-gears\n",
            "Successfully installed anndata-0.10.8 array-api-compat-1.8 cell-gears-0.1.2 dcor-0.6 legacy-api-wrap-1.4 pynndescent-0.5.13 scanpy-1.10.2 session-info-1.0.0 stdlib_list-0.10.0 umap-learn-0.5.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scgpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JhBbP0XiwQL",
        "outputId": "488a9915-9739-4eaa-8cec-bb3478ed87c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scgpt\n",
            "  Downloading scgpt-0.2.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting cell-gears<0.0.3 (from scgpt)\n",
            "  Downloading cell-gears-0.0.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets<3.0.0,>=2.3.0 (from scgpt)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting leidenalg>=0.8.10 (from scgpt)\n",
            "  Downloading leidenalg-0.10.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numba>=0.55.1 in /usr/local/lib/python3.10/dist-packages (from scgpt) (0.58.1)\n",
            "Collecting orbax<0.1.8 (from scgpt)\n",
            "  Downloading orbax-0.1.7-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.10/dist-packages (from scgpt) (2.0.3)\n",
            "Requirement already satisfied: scanpy<2.0.0,>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from scgpt) (1.10.2)\n",
            "Collecting scib<2.0.0,>=1.0.3 (from scgpt)\n",
            "  Downloading scib-1.1.5-1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting scikit-misc>=0.1.4 (from scgpt)\n",
            "  Downloading scikit_misc-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting scvi-tools<1.0,>=0.16.0 (from scgpt)\n",
            "  Downloading scvi_tools-0.20.3-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting torch<2.2,>=1.13.0 (from scgpt)\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (from scgpt) (0.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from scgpt) (4.12.2)\n",
            "Requirement already satisfied: umap-learn>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from scgpt) (0.5.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cell-gears<0.0.3->scgpt) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cell-gears<0.0.3->scgpt) (4.66.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from cell-gears<0.0.3->scgpt) (1.2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from cell-gears<0.0.3->scgpt) (3.3)\n",
            "Requirement already satisfied: dcor in /usr/local/lib/python3.10/dist-packages (from cell-gears<0.0.3->scgpt) (0.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.3.0->scgpt) (3.15.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets<3.0.0,>=2.3.0->scgpt)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.3.0->scgpt) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<3.0.0,>=2.3.0->scgpt)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting requests>=2.32.2 (from datasets<3.0.0,>=2.3.0->scgpt)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting xxhash (from datasets<3.0.0,>=2.3.0->scgpt)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets<3.0.0,>=2.3.0->scgpt)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets<3.0.0,>=2.3.0->scgpt) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.3.0->scgpt) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.3.0->scgpt) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.3.0->scgpt) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets<3.0.0,>=2.3.0->scgpt) (6.0.1)\n",
            "Collecting igraph<0.12,>=0.10.0 (from leidenalg>=0.8.10->scgpt)\n",
            "  Downloading igraph-0.11.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.55.1->scgpt) (0.41.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from orbax<0.1.8->scgpt) (1.4.0)\n",
            "Collecting cached_property (from orbax<0.1.8->scgpt)\n",
            "  Downloading cached_property-1.5.2-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from orbax<0.1.8->scgpt) (6.4.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from orbax<0.1.8->scgpt) (1.0.8)\n",
            "Requirement already satisfied: etils in /usr/local/lib/python3.10/dist-packages (from orbax<0.1.8->scgpt) (1.7.0)\n",
            "Requirement already satisfied: jax>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from orbax<0.1.8->scgpt) (0.4.26)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from orbax<0.1.8->scgpt) (0.4.26+cuda12.cudnn89)\n",
            "Requirement already satisfied: tensorstore>=0.1.20 in /usr/local/lib/python3.10/dist-packages (from orbax<0.1.8->scgpt) (0.1.45)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax<0.1.8->scgpt) (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->scgpt) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->scgpt) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->scgpt) (2024.1)\n",
            "Requirement already satisfied: anndata>=0.8 in /usr/local/lib/python3.10/dist-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (0.10.8)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (3.9.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (1.4.2)\n",
            "Requirement already satisfied: legacy-api-wrap>=1.4 in /usr/local/lib/python3.10/dist-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (1.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (3.7.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (8.4.0)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (0.5.6)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (0.5.13)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (1.11.4)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (0.13.1)\n",
            "Requirement already satisfied: session-info in /usr/local/lib/python3.10/dist-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (1.0.0)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy<2.0.0,>=1.9.1->scgpt) (0.14.2)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (from scib<2.0.0,>=1.0.3->scgpt) (1.4.2)\n",
            "Collecting deprecated (from scib<2.0.0,>=1.0.3->scgpt)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: chex in /usr/local/lib/python3.10/dist-packages (from scvi-tools<1.0,>=0.16.0->scgpt) (0.1.86)\n",
            "Collecting docrep>=0.3.2 (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
            "  Downloading docrep-0.3.2.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (from scvi-tools<1.0,>=0.16.0->scgpt) (0.8.4)\n",
            "Collecting ml-collections>=0.1.1 (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mudata>=0.1.2 (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
            "  Downloading mudata-0.2.4-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting numpyro (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
            "  Downloading numpyro-0.15.1-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: openpyxl>=3.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools<1.0,>=0.16.0->scgpt) (3.1.5)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from scvi-tools<1.0,>=0.16.0->scgpt) (0.2.2)\n",
            "Collecting pyro-ppl>=1.6.0 (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
            "  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting pytorch-lightning<1.10.0,>=1.9.0 (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
            "  Downloading pytorch_lightning-1.9.5-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from scvi-tools<1.0,>=0.16.0->scgpt) (13.7.1)\n",
            "Collecting torchmetrics>=0.11.0 (from scvi-tools<1.0,>=0.16.0->scgpt)\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.13.0->scgpt) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.13.0->scgpt) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.13.0->scgpt) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.13.0->scgpt) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.13.0->scgpt) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.13.0->scgpt) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.13.0->scgpt) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.13.0->scgpt) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.13.0->scgpt) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.13.0->scgpt) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.13.0->scgpt) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch<2.2,>=1.13.0->scgpt)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.13.0->scgpt) (12.1.105)\n",
            "Collecting triton==2.1.0 (from torch<2.2,>=1.13.0->scgpt)\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.2,>=1.13.0->scgpt) (12.5.82)\n",
            "INFO: pip is looking at multiple versions of torchtext to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchtext (from scgpt)\n",
            "  Downloading torchtext-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "  Downloading torchtext-0.17.1-cp310-cp310-manylinux1_x86_64.whl.metadata (7.6 kB)\n",
            "  Downloading torchtext-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.6 kB)\n",
            "  Downloading torchtext-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting torchdata==0.7.1 (from torchtext->scgpt)\n",
            "  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext->scgpt) (2.0.7)\n",
            "Requirement already satisfied: array-api-compat!=1.5,>1.4 in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy<2.0.0,>=1.9.1->scgpt) (1.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy<2.0.0,>=1.9.1->scgpt) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from docrep>=0.3.2->scvi-tools<1.0,>=0.16.0->scgpt) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scgpt) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scgpt) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scgpt) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scgpt) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scgpt) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<3.0.0,>=2.3.0->scgpt) (4.0.3)\n",
            "Collecting texttable>=1.6.2 (from igraph<0.12,>=0.10.0->leidenalg>=0.8.10->scgpt)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.6->orbax<0.1.8->scgpt) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.6->orbax<0.1.8->scgpt) (3.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scgpt) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scgpt) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scgpt) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scgpt) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scgpt) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy<2.0.0,>=1.9.1->scgpt) (3.1.2)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml-collections>=0.1.1->scvi-tools<1.0,>=0.16.0->scgpt) (21.6.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl>=3.0->scvi-tools<1.0,>=0.16.0->scgpt) (1.1.0)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl>=1.6.0->scvi-tools<1.0,>=0.16.0->scgpt)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting lightning-utilities>=0.6.0.post0 (from pytorch-lightning<1.10.0,>=1.9.0->scvi-tools<1.0,>=0.16.0->scgpt)\n",
            "  Downloading lightning_utilities-0.11.6-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<3.0.0,>=2.3.0->scgpt) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<3.0.0,>=2.3.0->scgpt) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<3.0.0,>=2.3.0->scgpt) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->scvi-tools<1.0,>=0.16.0->scgpt) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->scvi-tools<1.0,>=0.16.0->scgpt) (2.16.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->cell-gears<0.0.3->scgpt) (3.5.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex->scvi-tools<1.0,>=0.16.0->scgpt) (0.12.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->scib<2.0.0,>=1.0.3->scgpt) (1.14.1)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax->scvi-tools<1.0,>=0.16.0->scgpt) (0.4.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.2,>=1.13.0->scgpt) (2.1.5)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from numpyro->scvi-tools<1.0,>=0.16.0->scgpt) (1.0.0)\n",
            "Requirement already satisfied: stdlib-list in /usr/local/lib/python3.10/dist-packages (from session-info->scanpy<2.0.0,>=1.9.1->scgpt) (0.10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.2,>=1.13.0->scgpt) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.6.0.post0->pytorch-lightning<1.10.0,>=1.9.0->scvi-tools<1.0,>=0.16.0->scgpt) (71.0.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->scvi-tools<1.0,>=0.16.0->scgpt) (0.1.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->scvi-tools<1.0,>=0.16.0->scgpt) (3.20.3)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->scvi-tools<1.0,>=0.16.0->scgpt) (3.19.2)\n",
            "Downloading scgpt-0.2.1-py3-none-any.whl (829 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.2/829.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading leidenalg-0.10.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orbax-0.1.7-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scib-1.1.5-1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_misc-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scvi_tools-0.20.3-py3-none-any.whl (330 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.3/330.3 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.16.2-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading igraph-0.11.6-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mudata-0.2.4-py3-none-any.whl (24 kB)\n",
            "Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-1.9.5-py3-none-any.whl (829 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpyro-0.15.1-py3-none-any.whl (348 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.2/348.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.6-py3-none-any.whl (26 kB)\n",
            "Downloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: cell-gears, docrep, ml-collections\n",
            "  Building wheel for cell-gears (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cell-gears: filename=cell_gears-0.0.2-py3-none-any.whl size=27809 sha256=ffe70f073cba9403aebb16eaea0de13e8ddc2219b5fbf600b5a7249aff8795f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/b3/b0/c74d31453a56cd3410980f663cd971fce8ab1ad68e969a5aa2\n",
            "  Building wheel for docrep (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docrep: filename=docrep-0.3.2-py3-none-any.whl size=19876 sha256=653d71168084d3f6bb1bb2bb81ce4e282f3690e22a7634b30809616e62bf59ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/64/48/03c38d8d906159eaa210b3c548fdb590eb3e2a4a5745ae2172\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94506 sha256=73f50c6d5260072f7ab22154f226524ea1f119f7e070ec8bd09ced127f44e23b\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n",
            "Successfully built cell-gears docrep ml-collections\n",
            "Installing collected packages: texttable, pyro-api, cached_property, xxhash, triton, scikit-misc, requests, pyarrow, nvidia-nccl-cu12, ml-collections, lightning-utilities, igraph, docrep, dill, deprecated, multiprocess, leidenalg, torch, orbax, numpyro, torchmetrics, torchdata, pyro-ppl, mudata, datasets, torchtext, pytorch-lightning, scvi-tools, scib, cell-gears, scgpt\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.1\n",
            "    Uninstalling triton-2.3.1:\n",
            "      Successfully uninstalled triton-2.3.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.1+cu121\n",
            "    Uninstalling torch-2.3.1+cu121:\n",
            "      Successfully uninstalled torch-2.3.1+cu121\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.18.0\n",
            "    Uninstalling torchtext-0.18.0:\n",
            "      Successfully uninstalled torchtext-0.18.0\n",
            "  Attempting uninstall: cell-gears\n",
            "    Found existing installation: cell-gears 0.1.2\n",
            "    Uninstalling cell-gears-0.1.2:\n",
            "      Successfully uninstalled cell-gears-0.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\n",
            "torchaudio 2.3.1+cu121 requires torch==2.3.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.18.1+cu121 requires torch==2.3.1, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cached_property-1.5.2 cell-gears-0.0.2 datasets-2.20.0 deprecated-1.2.14 dill-0.3.8 docrep-0.3.2 igraph-0.11.6 leidenalg-0.10.2 lightning-utilities-0.11.6 ml-collections-0.1.1 mudata-0.2.4 multiprocess-0.70.16 numpyro-0.15.1 nvidia-nccl-cu12-2.18.1 orbax-0.1.7 pyarrow-17.0.0 pyro-api-0.1.2 pyro-ppl-1.9.1 pytorch-lightning-1.9.5 requests-2.32.3 scgpt-0.2.1 scib-1.1.5 scikit-misc-0.4.0 scvi-tools-0.20.3 texttable-1.7.0 torch-2.1.2 torchdata-0.7.1 torchmetrics-1.4.0.post0 torchtext-0.16.2 triton-2.1.0 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExtBovgmnXUk",
        "outputId": "faad4e2c-e2fe-4c98-d01a-271633084dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/save/scGPT_human"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1cZiMuMn8gR",
        "outputId": "a08bf31d-f536-4802-8b56-cbb63950a6f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "args.json  best_model.pt  vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from typing import Iterable, List, Tuple, Dict, Union, Optional\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext._torchtext import (\n",
        "    Vocab as VocabPybind,\n",
        ")\n",
        "from torch_geometric.loader import DataLoader\n",
        "from gears import PertData, GEARS\n",
        "from gears.inference import compute_metrics, deeper_analysis, non_dropout_analysis\n",
        "from gears.utils import create_cell_graph_dataset_for_prediction\n",
        "\n",
        "#sys.path.insert(0, \"../\")\n",
        "\n",
        "import scgpt as scg\n",
        "from scgpt.model import TransformerGenerator\n",
        "from scgpt.loss import (\n",
        "    masked_mse_loss,\n",
        "    criterion_neg_log_bernoulli,\n",
        "    masked_relative_error,\n",
        ")\n",
        "from scgpt.tokenizer import tokenize_batch, pad_batch, tokenize_and_pad_batch\n",
        "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
        "from scgpt.utils import set_seed, map_raw_id_to_vocab_id\n",
        "\n",
        "matplotlib.rcParams[\"savefig.transparent\"] = False\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "set_seed(42)\n"
      ],
      "metadata": {
        "id": "kTcxoVYFXs0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f11224c-cea2-42e9-d8a5-6cf395dbe4aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:914: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _GenerativeAIImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n",
            "/usr/local/lib/python3.10/dist-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
            "  warnings.warn(\"flash_attn is not installed\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Settings"
      ],
      "metadata": {
        "id": "4r9fIhk4jvZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# settings for data prcocessing\n",
        "pad_token = \"<pad>\"\n",
        "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
        "pad_value = 0  # for padding values\n",
        "pert_pad_id = 2\n",
        "\n",
        "n_hvg = 0  # number of highly variable genes\n",
        "include_zero_gene = \"all\"  # include zero expr genes in training input, \"all\", \"batch-wise\", \"row-wise\", or False\n",
        "max_seq_len = 1536\n",
        "\n",
        "# settings for training\n",
        "MLM = True  # whether to use masked language modeling, currently it is always on.\n",
        "CLS = False  # celltype classification objective\n",
        "CCE = False  # Contrastive cell embedding objective\n",
        "MVC = False  # Masked value prediction for cell embedding\n",
        "ECS = False  # Elastic cell similarity objective\n",
        "cell_emb_style = \"cls\"\n",
        "mvc_decoder_style = \"inner product, detach\"\n",
        "amp = True\n",
        "load_model = \"/content/drive/MyDrive/save/scGPT_human\"\n",
        "load_param_prefixs = [\n",
        "    \"encoder\",\n",
        "    \"value_encoder\",\n",
        "    \"transformer_encoder\",\n",
        "]\n",
        "\n",
        "# settings for optimizer\n",
        "lr = 1e-4  # or 1e-4\n",
        "batch_size = 32\n",
        "eval_batch_size = 32\n",
        "epochs = 1 # original is 15\n",
        "schedule_interval = 1\n",
        "early_stop = 5\n",
        "\n",
        "# settings for the model\n",
        "embsize = 512  # embedding dimension\n",
        "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 12  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 8  # number of heads in nn.MultiheadAttention\n",
        "n_layers_cls = 3\n",
        "dropout = 0.2  # dropout probability\n",
        "use_fast_transformer = True  # whether to use fast transformer\n",
        "\n",
        "# logging\n",
        "log_interval = 100\n",
        "\n",
        "# dataset and evaluation choices\n",
        "data_name = \"adamson\"\n",
        "split = \"simulation\"\n",
        "if data_name == \"norman\":\n",
        "    perts_to_plot = [\"SAMD1+ZBTB1\"]\n",
        "elif data_name == \"adamson\":\n",
        "    perts_to_plot = [\"KCTD16+ctrl\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "kxlEqZkxYQfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = Path(f\"./save/dev_perturb_{data_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"saving to {save_dir}\")\n",
        "\n",
        "logger = scg.logger\n",
        "scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
        "# log running date and current git commit\n",
        "logger.info(f\"Running on {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sSKiXfFjzzk",
        "outputId": "2ee5853c-b1d5-480c-e29e-dab4ea142c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving to save/dev_perturb_adamson-Jul25-17-34\n",
            "scGPT - INFO - Running on 2024-07-25 17:34:23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pert_data = PertData(\"./data\")\n",
        "pert_data.load(data_name=data_name)\n",
        "pert_data.prepare_split(split=split, seed=1)\n",
        "pert_data.get_dataloader(batch_size=batch_size, test_batch_size=eval_batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwaGr3dtj3SE",
        "outputId": "ac3d34d3-03e1-4385-f0aa-48093a931b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "100%|██████████| 141M/141M [00:07<00:00, 19.4MiB/s]\n",
            "Extracting zip file...\n",
            "Done!\n",
            "Creating pyg object for each cell in the data...\n",
            "100%|██████████| 87/87 [01:48<00:00,  1.25s/it]\n",
            "Saving new dataset pyg object at ./data/adamson/data_pyg/cell_graphs.pkl\n",
            "Done!\n",
            "Creating new splits....\n",
            "Saving new splits at ./data/adamson/splits/adamson_simulation_1_0.75.pkl\n",
            "Simulation split test composition:\n",
            "combo_seen0:0\n",
            "combo_seen1:0\n",
            "combo_seen2:0\n",
            "unseen_single:22\n",
            "Done!\n",
            "Creating dataloaders....\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if load_model is not None:\n",
        "    model_dir = Path(load_model)\n",
        "    model_config_file = model_dir / \"args.json\"\n",
        "    model_file = model_dir / \"best_model.pt\"\n",
        "    vocab_file = model_dir / \"vocab.json\"\n",
        "\n",
        "    vocab = GeneVocab.from_file(vocab_file)\n",
        "    for s in special_tokens:\n",
        "        if s not in vocab:\n",
        "            vocab.append_token(s)\n",
        "\n",
        "    pert_data.adata.var[\"id_in_vocab\"] = [\n",
        "        1 if gene in vocab else -1 for gene in pert_data.adata.var[\"gene_name\"]\n",
        "    ]\n",
        "    gene_ids_in_vocab = np.array(pert_data.adata.var[\"id_in_vocab\"])\n",
        "    logger.info(\n",
        "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
        "        f\"in vocabulary of size {len(vocab)}.\"\n",
        "    )\n",
        "    genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
        "\n",
        "    # model\n",
        "    with open(model_config_file, \"r\") as f:\n",
        "        model_configs = json.load(f)\n",
        "    logger.info(\n",
        "        f\"Resume model from {model_file}, the model args will override the \"\n",
        "        f\"config {model_config_file}.\"\n",
        "    )\n",
        "    embsize = model_configs[\"embsize\"]\n",
        "    nhead = model_configs[\"nheads\"]\n",
        "    d_hid = model_configs[\"d_hid\"]\n",
        "    nlayers = model_configs[\"nlayers\"]\n",
        "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
        "else:\n",
        "    genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
        "    vocab = Vocab(\n",
        "        VocabPybind(genes + special_tokens, None)\n",
        "    )  # bidirectional lookup [gene <-> int]\n",
        "vocab.set_default_index(vocab[\"<pad>\"])\n",
        "gene_ids = np.array(\n",
        "    [vocab[gene] if gene in vocab else vocab[\"<pad>\"] for gene in genes], dtype=int\n",
        ")\n",
        "n_genes = len(genes)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvZAe2Uej8LI",
        "outputId": "3a8537d6-2d76-4053-bbbc-78cee7bb59d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scGPT - INFO - match 4399/5060 genes in vocabulary of size 60697.\n",
            "scGPT - INFO - Resume model from /content/drive/MyDrive/save/scGPT_human/best_model.pt, the model args will override the config /content/drive/MyDrive/save/scGPT_human/args.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create and train scGpt\n",
        "\n"
      ],
      "metadata": {
        "id": "gkjLm8RCok6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "ntokens = len(vocab)  # size of vocabulary\n",
        "model = TransformerGenerator(\n",
        "    ntokens,\n",
        "    embsize,\n",
        "    nhead,\n",
        "    d_hid,\n",
        "    nlayers,\n",
        "    nlayers_cls=n_layers_cls,\n",
        "    n_cls=1,\n",
        "    vocab=vocab,\n",
        "    dropout=dropout,\n",
        "    pad_token=pad_token,\n",
        "    pad_value=pad_value,\n",
        "    pert_pad_id=pert_pad_id,\n",
        "    use_fast_transformer=use_fast_transformer,\n",
        ")\n",
        "\n",
        "def load_pretrained_model(model, model_file, load_param_prefixs=None):\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_dict = torch.load(model_file)\n",
        "\n",
        "    if load_param_prefixs is not None:\n",
        "        pretrained_dict = {\n",
        "            k: v\n",
        "            for k, v in pretrained_dict.items()\n",
        "            if any([k.startswith(prefix) for prefix in load_param_prefixs])\n",
        "        }\n",
        "    else:\n",
        "        pretrained_dict = {\n",
        "            k: v\n",
        "            for k, v in pretrained_dict.items()\n",
        "            if k in model_dict and v.shape == model_dict[k].shape\n",
        "        }\n",
        "\n",
        "    for k, v in pretrained_dict.items():\n",
        "        logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
        "\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "if load_model is not None:\n",
        "    try:\n",
        "        load_pretrained_model(model, model_file, load_param_prefixs)\n",
        "        logger.info(f\"Loading model params from {model_file}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading model params: {e}\")\n",
        "\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQBQ3YfuknPD",
        "outputId": "00dc8239-6b7e-4431-89eb-3c9ba125e8c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using simple batchnorm instead of domain specific batchnorm\n",
            "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
            "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
            "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
            "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
            "scGPT - ERROR - Error loading model params: Error(s) in loading state_dict for TransformerGenerator:\n",
            "\tUnexpected key(s) in state_dict: \"transformer_encoder.layers.0.self_attn.Wqkv.weight\", \"transformer_encoder.layers.0.self_attn.Wqkv.bias\", \"transformer_encoder.layers.1.self_attn.Wqkv.weight\", \"transformer_encoder.layers.1.self_attn.Wqkv.bias\", \"transformer_encoder.layers.2.self_attn.Wqkv.weight\", \"transformer_encoder.layers.2.self_attn.Wqkv.bias\", \"transformer_encoder.layers.3.self_attn.Wqkv.weight\", \"transformer_encoder.layers.3.self_attn.Wqkv.bias\", \"transformer_encoder.layers.4.self_attn.Wqkv.weight\", \"transformer_encoder.layers.4.self_attn.Wqkv.bias\", \"transformer_encoder.layers.5.self_attn.Wqkv.weight\", \"transformer_encoder.layers.5.self_attn.Wqkv.bias\", \"transformer_encoder.layers.6.self_attn.Wqkv.weight\", \"transformer_encoder.layers.6.self_attn.Wqkv.bias\", \"transformer_encoder.layers.7.self_attn.Wqkv.weight\", \"transformer_encoder.layers.7.self_attn.Wqkv.bias\", \"transformer_encoder.layers.8.self_attn.Wqkv.weight\", \"transformer_encoder.layers.8.self_attn.Wqkv.bias\", \"transformer_encoder.layers.9.self_attn.Wqkv.weight\", \"transformer_encoder.layers.9.self_attn.Wqkv.bias\", \"transformer_encoder.layers.10.self_attn.Wqkv.weight\", \"transformer_encoder.layers.10.self_attn.Wqkv.bias\", \"transformer_encoder.layers.11.self_attn.Wqkv.weight\", \"transformer_encoder.layers.11.self_attn.Wqkv.bias\". \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerGenerator(\n",
              "  (encoder): GeneEncoder(\n",
              "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
              "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (value_encoder): ContinuousValueEncoder(\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
              "    (activation): ReLU()\n",
              "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (pert_encoder): Embedding(3, 512, padding_idx=2)\n",
              "  (bn): BatchNorm1d(512, eps=6.1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-11): 12 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): ExprDecoder(\n",
              "    (fc): Sequential(\n",
              "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (1): LeakyReLU(negative_slope=0.01)\n",
              "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (3): LeakyReLU(negative_slope=0.01)\n",
              "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (cls_decoder): ClsDecoder(\n",
              "    (_decoder): ModuleList(\n",
              "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (4): ReLU()\n",
              "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (out_layer): Linear(in_features=512, out_features=1, bias=True)\n",
              "  )\n",
              "  (sim): Similarity(\n",
              "    (cos): CosineSimilarity()\n",
              "  )\n",
              "  (creterion_cce): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = masked_mse_loss\n",
        "criterion_cls = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, schedule_interval, gamma=0.9)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
        "\n",
        "\n",
        "def train(model: nn.Module, train_loader: torch.utils.data.DataLoader) -> None:\n",
        "    \"\"\"\n",
        "    Train the model for one epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss, total_mse = 0.0, 0.0\n",
        "    start_time = time.time()\n",
        "\n",
        "    num_batches = len(train_loader)\n",
        "    for batch, batch_data in enumerate(train_loader):\n",
        "        batch_size = len(batch_data.y)\n",
        "        batch_data.to(device)\n",
        "        x: torch.Tensor = batch_data.x  # (batch_size * n_genes, 2)\n",
        "        ori_gene_values = x[:, 0].view(batch_size, n_genes)\n",
        "        pert_flags = x[:, 1].long().view(batch_size, n_genes)\n",
        "        target_gene_values = batch_data.y  # (batch_size, n_genes)\n",
        "\n",
        "        if include_zero_gene in [\"all\", \"batch-wise\"]:\n",
        "            if include_zero_gene == \"all\":\n",
        "                input_gene_ids = torch.arange(n_genes, device=device, dtype=torch.long)\n",
        "            else:\n",
        "                input_gene_ids = (\n",
        "                    ori_gene_values.nonzero()[:, 1].flatten().unique().sort()[0]\n",
        "                )\n",
        "            # sample input_gene_id\n",
        "            if len(input_gene_ids) > max_seq_len:\n",
        "                input_gene_ids = torch.randperm(len(input_gene_ids), device=device)[\n",
        "                    :max_seq_len\n",
        "                ]\n",
        "            input_values = ori_gene_values[:, input_gene_ids]\n",
        "            input_pert_flags = pert_flags[:, input_gene_ids]\n",
        "            target_values = target_gene_values[:, input_gene_ids]\n",
        "\n",
        "            mapped_input_gene_ids = map_raw_id_to_vocab_id(input_gene_ids, gene_ids)\n",
        "            mapped_input_gene_ids = mapped_input_gene_ids.repeat(batch_size, 1)\n",
        "\n",
        "            # src_key_padding_mask = mapped_input_gene_ids.eq(vocab[pad_token])\n",
        "            src_key_padding_mask = torch.zeros_like(\n",
        "                input_values, dtype=torch.bool, device=device\n",
        "            )\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=amp):\n",
        "            output_dict = model(\n",
        "                mapped_input_gene_ids,\n",
        "                input_values,\n",
        "                input_pert_flags,\n",
        "                src_key_padding_mask=src_key_padding_mask,\n",
        "                CLS=CLS,\n",
        "                CCE=CCE,\n",
        "                MVC=MVC,\n",
        "                ECS=ECS,\n",
        "            )\n",
        "            output_values = output_dict[\"mlm_output\"]\n",
        "\n",
        "            masked_positions = torch.ones_like(\n",
        "                input_values, dtype=torch.bool\n",
        "            )  # Use all\n",
        "            loss = loss_mse = criterion(output_values, target_values, masked_positions)\n",
        "\n",
        "        model.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        with warnings.catch_warnings(record=True) as w:\n",
        "            warnings.filterwarnings(\"always\")\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                model.parameters(),\n",
        "                1.0,\n",
        "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
        "            )\n",
        "            if len(w) > 0:\n",
        "                logger.warning(\n",
        "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
        "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
        "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
        "                )\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # torch.cuda.empty_cache()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_mse += loss_mse.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            cur_mse = total_mse / log_interval\n",
        "            # ppl = math.exp(cur_loss)\n",
        "            logger.info(\n",
        "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
        "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
        "                f\"loss {cur_loss:5.2f} | mse {cur_mse:5.2f} |\"\n",
        "            )\n",
        "            total_loss = 0\n",
        "            total_mse = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module, val_loader: torch.utils.data.DataLoader) -> float:\n",
        "    \"\"\"\n",
        "    Evaluate the model on the evaluation data.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_error = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, batch_data in enumerate(val_loader):\n",
        "            batch_size = len(batch_data.y)\n",
        "            batch_data.to(device)\n",
        "            x: torch.Tensor = batch_data.x  # (batch_size * n_genes, 2)\n",
        "            ori_gene_values = x[:, 0].view(batch_size, n_genes)\n",
        "            pert_flags = x[:, 1].long().view(batch_size, n_genes)\n",
        "            target_gene_values = batch_data.y  # (batch_size, n_genes)\n",
        "\n",
        "            if include_zero_gene in [\"all\", \"batch-wise\"]:\n",
        "                if include_zero_gene == \"all\":\n",
        "                    input_gene_ids = torch.arange(n_genes, device=device)\n",
        "                else:  # when batch-wise\n",
        "                    input_gene_ids = (\n",
        "                        ori_gene_values.nonzero()[:, 1].flatten().unique().sort()[0]\n",
        "                    )\n",
        "\n",
        "                # sample input_gene_id\n",
        "                if len(input_gene_ids) > max_seq_len:\n",
        "                    input_gene_ids = torch.randperm(len(input_gene_ids), device=device)[\n",
        "                        :max_seq_len\n",
        "                    ]\n",
        "                input_values = ori_gene_values[:, input_gene_ids]\n",
        "                input_pert_flags = pert_flags[:, input_gene_ids]\n",
        "                target_values = target_gene_values[:, input_gene_ids]\n",
        "\n",
        "                mapped_input_gene_ids = map_raw_id_to_vocab_id(input_gene_ids, gene_ids)\n",
        "                mapped_input_gene_ids = mapped_input_gene_ids.repeat(batch_size, 1)\n",
        "\n",
        "                # src_key_padding_mask = mapped_input_gene_ids.eq(vocab[pad_token])\n",
        "                src_key_padding_mask = torch.zeros_like(\n",
        "                    input_values, dtype=torch.bool, device=input_values.device\n",
        "                )\n",
        "            with torch.cuda.amp.autocast(enabled=amp):\n",
        "                output_dict = model(\n",
        "                    mapped_input_gene_ids,\n",
        "                    input_values,\n",
        "                    input_pert_flags,\n",
        "                    src_key_padding_mask=src_key_padding_mask,\n",
        "                    CLS=CLS,\n",
        "                    CCE=CCE,\n",
        "                    MVC=MVC,\n",
        "                    ECS=ECS,\n",
        "                    do_sample=True,\n",
        "                )\n",
        "                output_values = output_dict[\"mlm_output\"]\n",
        "\n",
        "                masked_positions = torch.ones_like(\n",
        "                    input_values, dtype=torch.bool, device=input_values.device\n",
        "                )\n",
        "                loss = criterion(output_values, target_values, masked_positions)\n",
        "            total_loss += loss.item()\n",
        "            total_error += masked_relative_error(\n",
        "                output_values, target_values, masked_positions\n",
        "            ).item()\n",
        "    return total_loss / len(val_loader), total_error / len(val_loader)\n"
      ],
      "metadata": {
        "id": "i7ZpjHH1o9ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric"
      ],
      "metadata": {
        "id": "u7jeIb5l1drF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "best_model = None\n",
        "patience = 0\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train_loader = pert_data.dataloader[\"train_loader\"]\n",
        "    new_batch_size = train_loader.batch_size // 16  # Reduce batch size by half\n",
        "    train_loader = torch_geometric.loader.DataLoader(train_loader.dataset, batch_size=new_batch_size, shuffle=True)  # Create a new DataLoader with reduced batch size\n",
        "\n",
        "\n",
        "\n",
        "    valid_loader = pert_data.dataloader[\"val_loader\"]\n",
        "    new_batch_size = valid_loader.batch_size // 16  # Reduce batch size by half\n",
        "    valid_loader = torch_geometric.loader.DataLoader(valid_loader.dataset, batch_size=new_batch_size, shuffle=False)  # Create a new DataLoader with reduced batch size\n",
        "\n",
        "\n",
        "\n",
        "    train(\n",
        "        model,\n",
        "        train_loader,\n",
        "    )\n",
        "    val_loss, val_mre = evaluate(\n",
        "        model,\n",
        "        valid_loader,\n",
        "    )\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    logger.info(\"-\" * 89)\n",
        "    logger.info(\n",
        "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
        "        f\"valid loss/mse {val_loss:5.4f} |\"\n",
        "    )\n",
        "    logger.info(\"-\" * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
        "        patience = 0\n",
        "    else:\n",
        "        patience += 1\n",
        "        if patience >= early_stop:\n",
        "            logger.info(f\"Early stop at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    torch.save(\n",
        "        model.state_dict(),\n",
        "        save_dir / f\"model_{epoch}.pt\",\n",
        "    )\n",
        "\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_0zEWvXosyf",
        "outputId": "35980ebc-066f-4a54-9f6e-7c1a1b01c845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scGPT - INFO - | epoch   1 | 100/27180 batches | lr 0.0001 | ms/batch 53.00 | loss  0.27 | mse  0.27 |\n",
            "scGPT - INFO - | epoch   1 | 200/27180 batches | lr 0.0001 | ms/batch 42.00 | loss  0.11 | mse  0.11 |\n",
            "scGPT - INFO - | epoch   1 | 300/27180 batches | lr 0.0001 | ms/batch 41.47 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 400/27180 batches | lr 0.0001 | ms/batch 41.51 | loss  0.10 | mse  0.10 |\n",
            "scGPT - INFO - | epoch   1 | 500/27180 batches | lr 0.0001 | ms/batch 42.25 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 600/27180 batches | lr 0.0001 | ms/batch 41.81 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 700/27180 batches | lr 0.0001 | ms/batch 41.88 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 800/27180 batches | lr 0.0001 | ms/batch 42.19 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 900/27180 batches | lr 0.0001 | ms/batch 41.82 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 1000/27180 batches | lr 0.0001 | ms/batch 41.98 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 1100/27180 batches | lr 0.0001 | ms/batch 42.11 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 1200/27180 batches | lr 0.0001 | ms/batch 42.10 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 1300/27180 batches | lr 0.0001 | ms/batch 42.04 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 1400/27180 batches | lr 0.0001 | ms/batch 41.81 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 1500/27180 batches | lr 0.0001 | ms/batch 41.96 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 1600/27180 batches | lr 0.0001 | ms/batch 42.62 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 1700/27180 batches | lr 0.0001 | ms/batch 42.15 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 1800/27180 batches | lr 0.0001 | ms/batch 41.70 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 1900/27180 batches | lr 0.0001 | ms/batch 41.91 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 2000/27180 batches | lr 0.0001 | ms/batch 43.07 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 2100/27180 batches | lr 0.0001 | ms/batch 41.58 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 2200/27180 batches | lr 0.0001 | ms/batch 41.74 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 2300/27180 batches | lr 0.0001 | ms/batch 41.52 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 2400/27180 batches | lr 0.0001 | ms/batch 41.67 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 2500/27180 batches | lr 0.0001 | ms/batch 42.00 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 2600/27180 batches | lr 0.0001 | ms/batch 41.70 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 2700/27180 batches | lr 0.0001 | ms/batch 41.84 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 2800/27180 batches | lr 0.0001 | ms/batch 42.13 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 2900/27180 batches | lr 0.0001 | ms/batch 41.51 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 3000/27180 batches | lr 0.0001 | ms/batch 41.35 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 3100/27180 batches | lr 0.0001 | ms/batch 42.81 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 3200/27180 batches | lr 0.0001 | ms/batch 41.35 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 3300/27180 batches | lr 0.0001 | ms/batch 41.51 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 3400/27180 batches | lr 0.0001 | ms/batch 41.91 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 3500/27180 batches | lr 0.0001 | ms/batch 41.64 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 3600/27180 batches | lr 0.0001 | ms/batch 41.69 | loss  0.10 | mse  0.10 |\n",
            "scGPT - INFO - | epoch   1 | 3700/27180 batches | lr 0.0001 | ms/batch 42.23 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 3800/27180 batches | lr 0.0001 | ms/batch 41.84 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 3900/27180 batches | lr 0.0001 | ms/batch 41.53 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 4000/27180 batches | lr 0.0001 | ms/batch 42.24 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 4100/27180 batches | lr 0.0001 | ms/batch 41.84 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 4200/27180 batches | lr 0.0001 | ms/batch 41.64 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 4300/27180 batches | lr 0.0001 | ms/batch 42.79 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 4400/27180 batches | lr 0.0001 | ms/batch 41.86 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 4500/27180 batches | lr 0.0001 | ms/batch 42.22 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 4600/27180 batches | lr 0.0001 | ms/batch 42.00 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 4700/27180 batches | lr 0.0001 | ms/batch 41.91 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 4800/27180 batches | lr 0.0001 | ms/batch 41.86 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 4900/27180 batches | lr 0.0001 | ms/batch 41.77 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 5000/27180 batches | lr 0.0001 | ms/batch 41.49 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 5100/27180 batches | lr 0.0001 | ms/batch 41.63 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 5200/27180 batches | lr 0.0001 | ms/batch 41.50 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 5300/27180 batches | lr 0.0001 | ms/batch 41.55 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 5400/27180 batches | lr 0.0001 | ms/batch 41.34 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 5500/27180 batches | lr 0.0001 | ms/batch 41.90 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 5600/27180 batches | lr 0.0001 | ms/batch 41.78 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 5700/27180 batches | lr 0.0001 | ms/batch 41.55 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 5800/27180 batches | lr 0.0001 | ms/batch 41.96 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 5900/27180 batches | lr 0.0001 | ms/batch 41.87 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 6000/27180 batches | lr 0.0001 | ms/batch 42.14 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 6100/27180 batches | lr 0.0001 | ms/batch 42.16 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 6200/27180 batches | lr 0.0001 | ms/batch 41.88 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 6300/27180 batches | lr 0.0001 | ms/batch 41.96 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 6400/27180 batches | lr 0.0001 | ms/batch 42.37 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 6500/27180 batches | lr 0.0001 | ms/batch 41.83 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 6600/27180 batches | lr 0.0001 | ms/batch 41.83 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 6700/27180 batches | lr 0.0001 | ms/batch 42.38 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 6800/27180 batches | lr 0.0001 | ms/batch 41.92 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 6900/27180 batches | lr 0.0001 | ms/batch 42.18 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 7000/27180 batches | lr 0.0001 | ms/batch 42.02 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 7100/27180 batches | lr 0.0001 | ms/batch 41.64 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 7200/27180 batches | lr 0.0001 | ms/batch 41.80 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 7300/27180 batches | lr 0.0001 | ms/batch 41.51 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 7400/27180 batches | lr 0.0001 | ms/batch 41.78 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 7500/27180 batches | lr 0.0001 | ms/batch 42.45 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 7600/27180 batches | lr 0.0001 | ms/batch 42.11 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 7700/27180 batches | lr 0.0001 | ms/batch 41.22 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 7800/27180 batches | lr 0.0001 | ms/batch 41.87 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 7900/27180 batches | lr 0.0001 | ms/batch 41.90 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 8000/27180 batches | lr 0.0001 | ms/batch 41.49 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 8100/27180 batches | lr 0.0001 | ms/batch 42.26 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 8200/27180 batches | lr 0.0001 | ms/batch 41.52 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 8300/27180 batches | lr 0.0001 | ms/batch 41.71 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 8400/27180 batches | lr 0.0001 | ms/batch 42.85 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 8500/27180 batches | lr 0.0001 | ms/batch 41.92 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 8600/27180 batches | lr 0.0001 | ms/batch 41.59 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 8700/27180 batches | lr 0.0001 | ms/batch 42.99 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 8800/27180 batches | lr 0.0001 | ms/batch 41.80 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 8900/27180 batches | lr 0.0001 | ms/batch 42.09 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 9000/27180 batches | lr 0.0001 | ms/batch 42.36 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 9100/27180 batches | lr 0.0001 | ms/batch 41.09 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 9200/27180 batches | lr 0.0001 | ms/batch 41.48 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 9300/27180 batches | lr 0.0001 | ms/batch 42.63 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 9400/27180 batches | lr 0.0001 | ms/batch 41.75 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 9500/27180 batches | lr 0.0001 | ms/batch 41.55 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 9600/27180 batches | lr 0.0001 | ms/batch 41.83 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 9700/27180 batches | lr 0.0001 | ms/batch 41.58 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 9800/27180 batches | lr 0.0001 | ms/batch 41.76 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 9900/27180 batches | lr 0.0001 | ms/batch 42.76 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 10000/27180 batches | lr 0.0001 | ms/batch 42.07 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 10100/27180 batches | lr 0.0001 | ms/batch 41.84 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 10200/27180 batches | lr 0.0001 | ms/batch 41.80 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 10300/27180 batches | lr 0.0001 | ms/batch 41.38 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 10400/27180 batches | lr 0.0001 | ms/batch 41.98 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 10500/27180 batches | lr 0.0001 | ms/batch 42.48 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 10600/27180 batches | lr 0.0001 | ms/batch 41.86 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 10700/27180 batches | lr 0.0001 | ms/batch 42.12 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 10800/27180 batches | lr 0.0001 | ms/batch 42.17 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 10900/27180 batches | lr 0.0001 | ms/batch 41.42 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 11000/27180 batches | lr 0.0001 | ms/batch 42.03 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 11100/27180 batches | lr 0.0001 | ms/batch 42.03 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 11200/27180 batches | lr 0.0001 | ms/batch 41.64 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 11300/27180 batches | lr 0.0001 | ms/batch 41.67 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 11400/27180 batches | lr 0.0001 | ms/batch 41.73 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 11500/27180 batches | lr 0.0001 | ms/batch 41.60 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 11600/27180 batches | lr 0.0001 | ms/batch 42.56 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 11700/27180 batches | lr 0.0001 | ms/batch 41.44 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 11800/27180 batches | lr 0.0001 | ms/batch 41.96 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 11900/27180 batches | lr 0.0001 | ms/batch 42.29 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 12000/27180 batches | lr 0.0001 | ms/batch 41.81 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 12100/27180 batches | lr 0.0001 | ms/batch 41.74 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 12200/27180 batches | lr 0.0001 | ms/batch 41.83 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 12300/27180 batches | lr 0.0001 | ms/batch 41.56 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 12400/27180 batches | lr 0.0001 | ms/batch 41.47 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 12500/27180 batches | lr 0.0001 | ms/batch 42.24 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 12600/27180 batches | lr 0.0001 | ms/batch 41.55 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 12700/27180 batches | lr 0.0001 | ms/batch 42.13 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 12800/27180 batches | lr 0.0001 | ms/batch 41.83 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 12900/27180 batches | lr 0.0001 | ms/batch 41.68 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 13000/27180 batches | lr 0.0001 | ms/batch 41.74 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 13100/27180 batches | lr 0.0001 | ms/batch 42.31 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 13200/27180 batches | lr 0.0001 | ms/batch 41.53 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 13300/27180 batches | lr 0.0001 | ms/batch 41.99 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 13400/27180 batches | lr 0.0001 | ms/batch 42.28 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 13500/27180 batches | lr 0.0001 | ms/batch 41.64 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 13600/27180 batches | lr 0.0001 | ms/batch 41.35 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 13700/27180 batches | lr 0.0001 | ms/batch 41.97 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 13800/27180 batches | lr 0.0001 | ms/batch 42.69 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 13900/27180 batches | lr 0.0001 | ms/batch 41.38 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 14000/27180 batches | lr 0.0001 | ms/batch 42.53 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 14100/27180 batches | lr 0.0001 | ms/batch 41.79 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 14200/27180 batches | lr 0.0001 | ms/batch 41.79 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 14300/27180 batches | lr 0.0001 | ms/batch 42.32 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 14400/27180 batches | lr 0.0001 | ms/batch 41.97 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 14500/27180 batches | lr 0.0001 | ms/batch 41.74 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 14600/27180 batches | lr 0.0001 | ms/batch 41.91 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 14700/27180 batches | lr 0.0001 | ms/batch 41.39 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 14800/27180 batches | lr 0.0001 | ms/batch 41.91 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 14900/27180 batches | lr 0.0001 | ms/batch 41.60 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 15000/27180 batches | lr 0.0001 | ms/batch 41.83 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 15100/27180 batches | lr 0.0001 | ms/batch 41.76 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 15200/27180 batches | lr 0.0001 | ms/batch 41.81 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 15300/27180 batches | lr 0.0001 | ms/batch 42.24 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 15400/27180 batches | lr 0.0001 | ms/batch 41.85 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 15500/27180 batches | lr 0.0001 | ms/batch 41.45 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 15600/27180 batches | lr 0.0001 | ms/batch 41.58 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 15700/27180 batches | lr 0.0001 | ms/batch 43.12 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 15800/27180 batches | lr 0.0001 | ms/batch 41.46 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 15900/27180 batches | lr 0.0001 | ms/batch 41.53 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 16000/27180 batches | lr 0.0001 | ms/batch 43.22 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 16100/27180 batches | lr 0.0001 | ms/batch 42.58 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 16200/27180 batches | lr 0.0001 | ms/batch 41.46 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 16300/27180 batches | lr 0.0001 | ms/batch 43.12 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 16400/27180 batches | lr 0.0001 | ms/batch 41.97 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 16500/27180 batches | lr 0.0001 | ms/batch 41.54 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 16600/27180 batches | lr 0.0001 | ms/batch 42.10 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 16700/27180 batches | lr 0.0001 | ms/batch 41.69 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 16800/27180 batches | lr 0.0001 | ms/batch 42.28 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 16900/27180 batches | lr 0.0001 | ms/batch 41.84 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 17000/27180 batches | lr 0.0001 | ms/batch 41.52 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 17100/27180 batches | lr 0.0001 | ms/batch 41.52 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 17200/27180 batches | lr 0.0001 | ms/batch 42.84 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 17300/27180 batches | lr 0.0001 | ms/batch 41.53 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 17400/27180 batches | lr 0.0001 | ms/batch 41.94 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 17500/27180 batches | lr 0.0001 | ms/batch 41.82 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 17600/27180 batches | lr 0.0001 | ms/batch 41.96 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 17700/27180 batches | lr 0.0001 | ms/batch 42.78 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 17800/27180 batches | lr 0.0001 | ms/batch 42.45 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 17900/27180 batches | lr 0.0001 | ms/batch 42.09 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 18000/27180 batches | lr 0.0001 | ms/batch 42.19 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 18100/27180 batches | lr 0.0001 | ms/batch 42.48 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 18200/27180 batches | lr 0.0001 | ms/batch 41.66 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 18300/27180 batches | lr 0.0001 | ms/batch 42.17 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 18400/27180 batches | lr 0.0001 | ms/batch 42.08 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 18500/27180 batches | lr 0.0001 | ms/batch 41.74 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 18600/27180 batches | lr 0.0001 | ms/batch 41.94 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 18700/27180 batches | lr 0.0001 | ms/batch 41.80 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 18800/27180 batches | lr 0.0001 | ms/batch 41.55 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 18900/27180 batches | lr 0.0001 | ms/batch 42.04 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 19000/27180 batches | lr 0.0001 | ms/batch 41.90 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 19100/27180 batches | lr 0.0001 | ms/batch 41.94 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 19200/27180 batches | lr 0.0001 | ms/batch 42.78 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 19300/27180 batches | lr 0.0001 | ms/batch 41.84 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 19400/27180 batches | lr 0.0001 | ms/batch 41.49 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 19500/27180 batches | lr 0.0001 | ms/batch 41.71 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 19600/27180 batches | lr 0.0001 | ms/batch 41.37 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 19700/27180 batches | lr 0.0001 | ms/batch 41.53 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 19800/27180 batches | lr 0.0001 | ms/batch 42.64 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 19900/27180 batches | lr 0.0001 | ms/batch 42.17 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 20000/27180 batches | lr 0.0001 | ms/batch 41.48 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 20100/27180 batches | lr 0.0001 | ms/batch 42.64 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 20200/27180 batches | lr 0.0001 | ms/batch 41.54 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 20300/27180 batches | lr 0.0001 | ms/batch 41.71 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 20400/27180 batches | lr 0.0001 | ms/batch 41.86 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 20500/27180 batches | lr 0.0001 | ms/batch 41.46 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 20600/27180 batches | lr 0.0001 | ms/batch 42.01 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 20700/27180 batches | lr 0.0001 | ms/batch 42.82 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 20800/27180 batches | lr 0.0001 | ms/batch 41.88 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 20900/27180 batches | lr 0.0001 | ms/batch 41.94 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 21000/27180 batches | lr 0.0001 | ms/batch 42.51 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 21100/27180 batches | lr 0.0001 | ms/batch 41.75 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 21200/27180 batches | lr 0.0001 | ms/batch 41.47 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 21300/27180 batches | lr 0.0001 | ms/batch 42.20 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 21400/27180 batches | lr 0.0001 | ms/batch 41.98 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 21500/27180 batches | lr 0.0001 | ms/batch 41.68 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 21600/27180 batches | lr 0.0001 | ms/batch 42.01 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 21700/27180 batches | lr 0.0001 | ms/batch 41.72 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 21800/27180 batches | lr 0.0001 | ms/batch 41.83 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 21900/27180 batches | lr 0.0001 | ms/batch 41.85 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 22000/27180 batches | lr 0.0001 | ms/batch 41.70 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 22100/27180 batches | lr 0.0001 | ms/batch 42.06 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 22200/27180 batches | lr 0.0001 | ms/batch 41.58 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 22300/27180 batches | lr 0.0001 | ms/batch 41.78 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 22400/27180 batches | lr 0.0001 | ms/batch 41.79 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 22500/27180 batches | lr 0.0001 | ms/batch 42.13 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 22600/27180 batches | lr 0.0001 | ms/batch 41.42 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 22700/27180 batches | lr 0.0001 | ms/batch 42.26 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 22800/27180 batches | lr 0.0001 | ms/batch 41.94 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 22900/27180 batches | lr 0.0001 | ms/batch 41.51 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 23000/27180 batches | lr 0.0001 | ms/batch 42.39 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 23100/27180 batches | lr 0.0001 | ms/batch 41.82 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 23200/27180 batches | lr 0.0001 | ms/batch 41.77 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 23300/27180 batches | lr 0.0001 | ms/batch 42.27 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 23400/27180 batches | lr 0.0001 | ms/batch 41.75 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 23500/27180 batches | lr 0.0001 | ms/batch 41.86 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 23600/27180 batches | lr 0.0001 | ms/batch 42.65 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 23700/27180 batches | lr 0.0001 | ms/batch 41.77 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 23800/27180 batches | lr 0.0001 | ms/batch 42.04 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 23900/27180 batches | lr 0.0001 | ms/batch 42.31 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 24000/27180 batches | lr 0.0001 | ms/batch 42.09 | loss  0.09 | mse  0.09 |\n",
            "scGPT - INFO - | epoch   1 | 24100/27180 batches | lr 0.0001 | ms/batch 41.84 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 24200/27180 batches | lr 0.0001 | ms/batch 42.44 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 24300/27180 batches | lr 0.0001 | ms/batch 42.03 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 24400/27180 batches | lr 0.0001 | ms/batch 41.40 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 24500/27180 batches | lr 0.0001 | ms/batch 41.75 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 24600/27180 batches | lr 0.0001 | ms/batch 41.45 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 24700/27180 batches | lr 0.0001 | ms/batch 41.81 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 24800/27180 batches | lr 0.0001 | ms/batch 42.37 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 24900/27180 batches | lr 0.0001 | ms/batch 41.64 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 25000/27180 batches | lr 0.0001 | ms/batch 42.31 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 25100/27180 batches | lr 0.0001 | ms/batch 42.05 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 25200/27180 batches | lr 0.0001 | ms/batch 41.71 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 25300/27180 batches | lr 0.0001 | ms/batch 41.32 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 25400/27180 batches | lr 0.0001 | ms/batch 42.54 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 25500/27180 batches | lr 0.0001 | ms/batch 41.48 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 25600/27180 batches | lr 0.0001 | ms/batch 42.32 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 25700/27180 batches | lr 0.0001 | ms/batch 41.93 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 25800/27180 batches | lr 0.0001 | ms/batch 41.77 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 25900/27180 batches | lr 0.0001 | ms/batch 42.23 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 26000/27180 batches | lr 0.0001 | ms/batch 42.65 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 26100/27180 batches | lr 0.0001 | ms/batch 41.32 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 26200/27180 batches | lr 0.0001 | ms/batch 41.58 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 26300/27180 batches | lr 0.0001 | ms/batch 42.03 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 26400/27180 batches | lr 0.0001 | ms/batch 41.84 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 26500/27180 batches | lr 0.0001 | ms/batch 42.13 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 26600/27180 batches | lr 0.0001 | ms/batch 41.77 | loss  0.07 | mse  0.07 |\n",
            "scGPT - INFO - | epoch   1 | 26700/27180 batches | lr 0.0001 | ms/batch 41.37 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 26800/27180 batches | lr 0.0001 | ms/batch 41.99 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 26900/27180 batches | lr 0.0001 | ms/batch 41.71 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 27000/27180 batches | lr 0.0001 | ms/batch 42.35 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - | epoch   1 | 27100/27180 batches | lr 0.0001 | ms/batch 42.31 | loss  0.08 | mse  0.08 |\n",
            "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
            "scGPT - INFO - | end of epoch   1 | time: 1167.07s | valid loss/mse 0.1411 |\n",
            "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
            "scGPT - INFO - Best model with score 0.1411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(best_model.state_dict(), save_dir / \"best_model.pt\")\n"
      ],
      "metadata": {
        "id": "92QM65O4za1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluations"
      ],
      "metadata": {
        "id": "fqNvEmW_QxVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(\n",
        "    model: TransformerGenerator, pert_list: List[str], pool_size: Optional[int] = None\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Predict the gene expression values for the given perturbations.\n",
        "\n",
        "    Args:\n",
        "        model (:class:`torch.nn.Module`): The model to use for prediction.\n",
        "        pert_list (:obj:`List[str]`): The list of perturbations to predict.\n",
        "        pool_size (:obj:`int`, optional): For each perturbation, use this number\n",
        "            of cells in the control and predict their perturbation results. Report\n",
        "            the stats of these predictions. If `None`, use all control cells.\n",
        "    \"\"\"\n",
        "    adata = pert_data.adata\n",
        "    ctrl_adata = adata[adata.obs[\"condition\"] == \"ctrl\"]\n",
        "    if pool_size is None:\n",
        "        pool_size = len(ctrl_adata.obs)\n",
        "    gene_list = pert_data.gene_names.values.tolist()\n",
        "    for pert in pert_list:\n",
        "        for i in pert:\n",
        "            if i not in gene_list:\n",
        "                raise ValueError(\n",
        "                    \"The gene is not in the perturbation graph. Please select from GEARS.gene_list!\"\n",
        "                )\n",
        "\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    with torch.no_grad():\n",
        "        results_pred = {}\n",
        "        for pert in pert_list:\n",
        "            cell_graphs = create_cell_graph_dataset_for_prediction(\n",
        "                pert, ctrl_adata, gene_list, device, num_samples=pool_size\n",
        "            )\n",
        "            loader = DataLoader(cell_graphs, batch_size=eval_batch_size, shuffle=False)\n",
        "            preds = []\n",
        "            for batch_data in loader:\n",
        "                pred_gene_values = model.pred_perturb(\n",
        "                    batch_data, include_zero_gene, gene_ids=gene_ids, amp=amp\n",
        "                )\n",
        "                preds.append(pred_gene_values)\n",
        "            preds = torch.cat(preds, dim=0)\n",
        "            results_pred[\"_\".join(pert)] = np.mean(preds.detach().cpu().numpy(), axis=0)\n",
        "\n",
        "    return results_pred"
      ],
      "metadata": {
        "id": "sVCBJszzQukX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_perturbation(\n",
        "    model: nn.Module, query: str, save_file: str = None, pool_size: int = None\n",
        "):\n",
        "    import seaborn as sns\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    sns.set_theme(style=\"ticks\", rc={\"axes.facecolor\": (0, 0, 0, 0)}, font_scale=1.5)\n",
        "\n",
        "    adata = pert_data.adata\n",
        "    gene2idx = pert_data.node_map\n",
        "    cond2name = dict(adata.obs[[\"condition\", \"condition_name\"]].values)\n",
        "    gene_raw2id = dict(zip(adata.var.index.values, adata.var.gene_name.values))\n",
        "\n",
        "    de_idx = [\n",
        "        gene2idx[gene_raw2id[i]]\n",
        "        for i in adata.uns[\"top_non_dropout_de_20\"][cond2name[query]]\n",
        "    ]\n",
        "    genes = [\n",
        "        gene_raw2id[i] for i in adata.uns[\"top_non_dropout_de_20\"][cond2name[query]]\n",
        "    ]\n",
        "    truth = adata[adata.obs.condition == query].X.toarray()[:, de_idx]\n",
        "    if query.split(\"+\")[1] == \"ctrl\":\n",
        "        pred = predict(model, [[query.split(\"+\")[0]]], pool_size=pool_size)\n",
        "        pred = pred[query.split(\"+\")[0]][de_idx]\n",
        "    else:\n",
        "        pred = predict(model, [query.split(\"+\")], pool_size=pool_size)\n",
        "        pred = pred[\"_\".join(query.split(\"+\"))][de_idx]\n",
        "    ctrl_means = adata[adata.obs[\"condition\"] == \"ctrl\"].to_df().mean()[de_idx].values\n",
        "\n",
        "    pred = pred - ctrl_means\n",
        "    truth = truth - ctrl_means\n",
        "\n",
        "    plt.figure(figsize=[16.5, 4.5])\n",
        "    plt.title(query)\n",
        "    plt.boxplot(truth, showfliers=False, medianprops=dict(linewidth=0))\n",
        "\n",
        "    for i in range(pred.shape[0]):\n",
        "        _ = plt.scatter(i + 1, pred[i], color=\"red\")\n",
        "\n",
        "    plt.axhline(0, linestyle=\"dashed\", color=\"green\")\n",
        "\n",
        "    ax = plt.gca()\n",
        "    ax.xaxis.set_ticklabels(genes, rotation=90)\n",
        "\n",
        "    plt.ylabel(\"Change in Gene Expression over Control\", labelpad=10)\n",
        "    plt.tick_params(axis=\"x\", which=\"major\", pad=5)\n",
        "    plt.tick_params(axis=\"y\", which=\"major\", pad=5)\n",
        "    sns.despine()\n",
        "\n",
        "    if save_file:\n",
        "        plt.savefig(save_file, bbox_inches=\"tight\", transparent=False)\n",
        "    # plt.show()"
      ],
      "metadata": {
        "id": "UJ8Bkm_IQ2pK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict(best_model, [[\"FEV\"], [\"FEV\", \"SAMD11\"]])\n",
        "for p in perts_to_plot:\n",
        "    plot_perturbation(best_model, p, pool_size=300, save_file=f\"{save_dir}/{p}.png\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LEDlGRALQ6Jb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "a92d5280-aa7d-4dca-d489-57d424400bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1650x450 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_perturb(\n",
        "    loader: DataLoader, model: TransformerGenerator, device: torch.device\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Run model in inference mode using a given data loader\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    pert_cat = []\n",
        "    pred = []\n",
        "    truth = []\n",
        "    pred_de = []\n",
        "    truth_de = []\n",
        "    results = {}\n",
        "    logvar = []\n",
        "\n",
        "    for itr, batch in enumerate(loader):\n",
        "        batch.to(device)\n",
        "        pert_cat.extend(batch.pert)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            p = model.pred_perturb(batch, include_zero_gene, gene_ids=gene_ids)\n",
        "            t = batch.y\n",
        "            pred.extend(p.cpu())\n",
        "            truth.extend(t.cpu())\n",
        "\n",
        "            # Differentially expressed genes\n",
        "            for itr, de_idx in enumerate(batch.de_idx):\n",
        "                pred_de.append(p[itr, de_idx])\n",
        "                truth_de.append(t[itr, de_idx])\n",
        "\n",
        "    # all genes\n",
        "    results[\"pert_cat\"] = np.array(pert_cat)\n",
        "    pred = torch.stack(pred)\n",
        "    truth = torch.stack(truth)\n",
        "    results[\"pred\"] = pred.detach().cpu().numpy().astype(float)\n",
        "    results[\"truth\"] = truth.detach().cpu().numpy().astype(float)\n",
        "\n",
        "    pred_de = torch.stack(pred_de)\n",
        "    truth_de = torch.stack(truth_de)\n",
        "    results[\"pred_de\"] = pred_de.detach().cpu().numpy().astype(float)\n",
        "    results[\"truth_de\"] = truth_de.detach().cpu().numpy().astype(float)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Y3a9GOzUQ9ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = pert_data.dataloader[\"test_loader\"]\n",
        "test_res = eval_perturb(test_loader, best_model, device)\n",
        "test_metrics, test_pert_res = compute_metrics(test_res)\n",
        "print(test_metrics)\n",
        "\n",
        "# save the dicts in json\n",
        "with open(f\"{save_dir}/test_metrics.json\", \"w\") as f:\n",
        "    json.dump(test_metrics, f)\n",
        "with open(f\"{save_dir}/test_pert_res.json\", \"w\") as f:\n",
        "    json.dump(test_pert_res, f)\n",
        "\n",
        "deeper_res = deeper_analysis(pert_data.adata, test_res)\n",
        "non_dropout_res = non_dropout_analysis(pert_data.adata, test_res)\n",
        "\n",
        "metrics = [\"pearson_delta\", \"pearson_delta_de\"]\n",
        "metrics_non_dropout = [\n",
        "    \"pearson_delta_top20_de_non_dropout\",\n",
        "    \"pearson_top20_de_non_dropout\",\n",
        "]\n",
        "subgroup_analysis = {}\n",
        "for name in pert_data.subgroup[\"test_subgroup\"].keys():\n",
        "    subgroup_analysis[name] = {}\n",
        "    for m in metrics:\n",
        "        subgroup_analysis[name][m] = []\n",
        "\n",
        "    for m in metrics_non_dropout:\n",
        "        subgroup_analysis[name][m] = []\n",
        "\n",
        "for name, pert_list in pert_data.subgroup[\"test_subgroup\"].items():\n",
        "    for pert in pert_list:\n",
        "        for m in metrics:\n",
        "            subgroup_analysis[name][m].append(deeper_res[pert][m])\n",
        "\n",
        "        for m in metrics_non_dropout:\n",
        "            subgroup_analysis[name][m].append(non_dropout_res[pert][m])\n",
        "\n",
        "for name, result in subgroup_analysis.items():\n",
        "    for m in result.keys():\n",
        "        subgroup_analysis[name][m] = np.mean(subgroup_analysis[name][m])\n",
        "        logger.info(\"test_\" + name + \"_\" + m + \": \" + str(subgroup_analysis[name][m]))"
      ],
      "metadata": {
        "id": "aC6fGfmcRCv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67382568-04fc-44df-cbd6-5b77c3bc54ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'mse': 0.007370387258993897, 'mse_de': 0.1634758356063262, 'pearson': 0.9882317871448131, 'pearson_de': 0.9772086956369399}\n",
            "scGPT - INFO - test_combo_seen0_pearson_delta: nan\n",
            "scGPT - INFO - test_combo_seen0_pearson_delta_de: nan\n",
            "scGPT - INFO - test_combo_seen0_pearson_delta_top20_de_non_dropout: nan\n",
            "scGPT - INFO - test_combo_seen0_pearson_top20_de_non_dropout: nan\n",
            "scGPT - INFO - test_combo_seen1_pearson_delta: nan\n",
            "scGPT - INFO - test_combo_seen1_pearson_delta_de: nan\n",
            "scGPT - INFO - test_combo_seen1_pearson_delta_top20_de_non_dropout: nan\n",
            "scGPT - INFO - test_combo_seen1_pearson_top20_de_non_dropout: nan\n",
            "scGPT - INFO - test_combo_seen2_pearson_delta: nan\n",
            "scGPT - INFO - test_combo_seen2_pearson_delta_de: nan\n",
            "scGPT - INFO - test_combo_seen2_pearson_delta_top20_de_non_dropout: nan\n",
            "scGPT - INFO - test_combo_seen2_pearson_top20_de_non_dropout: nan\n",
            "scGPT - INFO - test_unseen_single_pearson_delta: 0.5605296157151005\n",
            "scGPT - INFO - test_unseen_single_pearson_delta_de: 0.7513880171917733\n",
            "scGPT - INFO - test_unseen_single_pearson_delta_top20_de_non_dropout: 0.7528723777079566\n",
            "scGPT - INFO - test_unseen_single_pearson_top20_de_non_dropout: 0.9744602431269535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "smITolCoE6VI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}